{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer, RobertaModel, RobertaTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AdamW, get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "statement_df = pd.read_csv('all-data.csv',encoding='UTF-8',header=None)\n",
    "statement_df.columns = ['sentiment','statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                          statement\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device for model training\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8dd0ebcc50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seeds (there will still be some random variability though)\n",
    "\n",
    "RANDOM_SEED = 73\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean statements\n",
    "\n",
    "def clean_statements(statement):\n",
    "    statement = re.sub(\" '\", \"'\", statement)\n",
    "    statement = re.sub(\" 's\", \"'s\", statement)\n",
    "    statement = re.sub('\\( ', '(', statement)\n",
    "    statement = re.sub(' \\)', ')', statement)\n",
    "    statement = re.sub('``', '\"', statement)\n",
    "    statement = re.sub(\"''\", '\"', statement)\n",
    "    statement = re.sub(r'\\s([?.,%:!\"](?:\\s|$))', r'\\1', statement)\n",
    "    return statement\n",
    "\n",
    "statement_df['statement'] = statement_df['statement'].apply(clean_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran, the company has no plans to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company's updated strategy fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai's beer sales fell by 6.5 per cent t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5% in January...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                          statement\n",
       "0      neutral  According to Gran, the company has no plans to...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company's updated strategy fo...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai's beer sales fell by 6.5 per cent t...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5% in January...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode classes\n",
    "\n",
    "le = LabelEncoder()\n",
    "statement_df['sentiment'] = le.fit_transform(statement_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>statement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran, the company has no plans to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company's updated strategy fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>0</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>1</td>\n",
       "      <td>Rinkuskiai's beer sales fell by 6.5 per cent t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>0</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>0</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>0</td>\n",
       "      <td>Sales in Finland decreased by 10.5% in January...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                          statement\n",
       "0             1  According to Gran, the company has no plans to...\n",
       "1             1  Technopolis plans to develop in stages an area...\n",
       "2             0  The international electronic industry company ...\n",
       "3             2  With the new production plant the company woul...\n",
       "4             2  According to the company's updated strategy fo...\n",
       "...         ...                                                ...\n",
       "4841          0  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842          1  Rinkuskiai's beer sales fell by 6.5 per cent t...\n",
       "4843          0  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844          0  Net sales of the Paper segment decreased to EU...\n",
       "4845          0  Sales in Finland decreased by 10.5% in January...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 3\n",
    "EPOCHS = 5\n",
    "DROPOUT_PROB = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "NFOLDS = 10\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create helper function for time formatting\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/val/test sets (80/10/10) for single model training\n",
    "\n",
    "df_train,df_test = train_test_split(statement_df,\n",
    "                                    test_size = 0.2,\n",
    "                                    random_state=RANDOM_SEED,\n",
    "                                    stratify = statement_df['sentiment'].values)\n",
    "\n",
    "df_val,df_test = train_test_split(df_test,\n",
    "                                  test_size=0.5,\n",
    "                                  random_state=RANDOM_SEED,\n",
    "                                  stratify=df_test['sentiment'].values\n",
    "                                 )\n",
    "df_train_full = pd.concat([df_train,df_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dataset class and functions for creating datasets\n",
    "\n",
    "class StatementDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,statements,labels,tokenizer,max_length):\n",
    "        self.statements = statements\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.statements)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        statement = str(self.statements[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        enconding = self.tokenizer.encode_plus(statement,\n",
    "                                               max_length=self.max_length,\n",
    "                                               padding='max_length',\n",
    "                                               add_special_tokens=True, \n",
    "                                               return_token_type_ids=False,\n",
    "                                               truncation=True,\n",
    "                                               return_attention_mask=True,\n",
    "                                               return_tensors='pt'  \n",
    "                                              )\n",
    "        \n",
    "        return {\n",
    "            'statement_text': statement,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "def create_dataset(df,tokenizer,max_length):\n",
    "    ds = StatementDataset(statements=df['statement'].to_numpy(),\n",
    "                          labels = df['sentiment'].to_numpy(),\n",
    "                          tokenizer=tokenizer,\n",
    "                          max_length=max_length\n",
    "                         )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_dataloader(ds,batch_size):\n",
    "    return DataLoader(ds,batch_size,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for evaluating model accuracy\n",
    "\n",
    "def cv_ensemble_performance(preds,labels):\n",
    "    preds = np.array(preds)\n",
    "    summed = np.sum(preds,axis=0)\n",
    "    preds = np.argmax(summed,axis=1)\n",
    "    print(confusion_matrix(y_true=labels,y_pred=preds))\n",
    "    print('')\n",
    "    print(classification_report(y_true=labels,y_pred=preds,digits=3,target_names=le.classes_))\n",
    "\n",
    "def single_model_performance(preds,labels):\n",
    "    print(confusion_matrix(y_true=labels,y_pred=preds))\n",
    "    print('')\n",
    "    print(classification_report(y_true=labels,y_pred=preds,digits=3,target_names=le.classes_))\n",
    "    \n",
    "\n",
    "# create a function for fine tunning single model\n",
    "\n",
    "def train_model(model,device,data_loader,loss_function,optimizer,scheduler,n_examples):\n",
    "    \n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_preds=0\n",
    "    complete_preds=[]\n",
    "    complete_labels=[]\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        _,preds = torch.max(outputs,dim=1)\n",
    "        loss = loss_function(outputs,labels)\n",
    "        complete_preds.append(preds.data.cpu().numpy().tolist())\n",
    "        complete_labels.append(labels.data.cpu().numpy().tolist())\n",
    "        correct_preds += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    complete_preds_flat = [x for y in complete_preds for x in y]\n",
    "    complete_labels_flat = [x for y in complete_labels for x in y]\n",
    "    acc_score = accuracy_score(y_true=complete_labels_flat, \n",
    "                             y_pred=complete_preds_flat)\n",
    "    return acc_score, np.mean(losses)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating a single model\n",
    "def eval_model(model, device, data_loader, loss_function, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_preds = 0\n",
    "    complete_preds = []\n",
    "    complete_labels = []\n",
    "    complete_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for item in data_loader:\n",
    "            input_ids = item['input_ids'].to(device)\n",
    "            attention_mask = item['attention_mask'].to(device)\n",
    "            labels = item['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask)\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            correct_preds += torch.sum(preds == labels)\n",
    "            complete_preds.append(preds.data.cpu().numpy().tolist())\n",
    "            complete_labels.append(labels.data.cpu().numpy().tolist())\n",
    "            complete_outputs.append(outputs.tolist())\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        accuracy = correct_preds.double() / n_examples\n",
    "        complete_preds_flat = [x for y in complete_preds for x in y]\n",
    "        complete_labels_flat = [x for y in complete_labels for x in y]\n",
    "        complete_outputs_flat = [x for y in complete_outputs for x in y]\n",
    "\n",
    "        acc_score = accuracy_score(y_true=complete_labels_flat, \n",
    "                             y_pred=complete_preds_flat)\n",
    "        \n",
    "        return_items = (acc_score, \n",
    "                        np.mean(losses), \n",
    "                        complete_preds_flat, \n",
    "                        complete_outputs_flat)\n",
    "        \n",
    "        return return_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for completing a training fold for k-fold validation\n",
    "def train_fold(epochs, model, device, train_dataloader, \n",
    "               val_dataloader, test_dataloader, loss_fn, optimizer, \n",
    "               scheduler, model_save_name, n_train, n_val, single_model=True):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print('Epoch ', epoch+1, '/', epochs)\n",
    "        print('-'*50)\n",
    "\n",
    "        training_output = train_model(model, \n",
    "                                      device, \n",
    "                                      train_dataloader, \n",
    "                                      loss_fn, \n",
    "                                      optimizer, \n",
    "                                      scheduler, \n",
    "                                      n_train)\n",
    "        \n",
    "        train_acc, train_loss = training_output\n",
    "        \n",
    "        val_output = eval_model(model, \n",
    "                                device, \n",
    "                                val_dataloader, \n",
    "                                loss_fn, \n",
    "                                n_val)\n",
    "\n",
    "        val_acc, val_loss, val_preds, val_outputs = val_output\n",
    "        \n",
    "        history['train_accuracy'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_preds'].append(val_preds)\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), model_save_name)\n",
    "            best_accuracy = val_acc\n",
    "            best_preds = val_preds\n",
    "            best_outputs = val_outputs\n",
    "\n",
    "        print('Train Loss: ', \n",
    "              train_loss, \n",
    "              ' | ', \n",
    "              'Train Accuracy: ', \n",
    "              train_acc)\n",
    "        print('Val Loss: ', \n",
    "              val_loss, \n",
    "              ' | ', \n",
    "              'Val Accuracy: ', \n",
    "              val_acc)\n",
    "        print('Epoch Train Time: ', \n",
    "              format_time(time.time() - epoch_start_time))\n",
    "        print('\\n')\n",
    "    \n",
    "    print('Finished Training.')   \n",
    "    print('Fold Train Time: ', format_time(time.time() - start_time))\n",
    "    print('\\n')\n",
    "    if single_model:\n",
    "        _, _, test_preds, test_outputs = eval_model(model, \n",
    "                                                    device, \n",
    "                                                    test_dataloader, \n",
    "                                                    loss_function, \n",
    "                                                    len(df_test))\n",
    "        \n",
    "        single_model_performance(test_preds, df_test['sentiment'].values)\n",
    "    return history, best_preds, best_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for completing k-fold validation\n",
    "def get_oof_and_test_preds(model_type, tokenizer, \n",
    "                           train_df, test_df, single_model=False):\n",
    "    oof_preds = []\n",
    "    oof_outputs = []\n",
    "    oof_preds_indices = []\n",
    "    test_preds_list = []\n",
    "    test_outputs_list = []\n",
    "    history_list = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    fold = 0\n",
    "    \n",
    "    x_train = train_df['statement']\n",
    "    y_train = train_df['sentiment']\n",
    "\n",
    "    for train_index, val_index in skf.split(x_train, y_train):\n",
    "        print('Fold: {}'.format(fold+1))\n",
    "        \n",
    "        x_tr = x_train.iloc[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_va = x_train.iloc[val_index]\n",
    "        y_va = y_train.iloc[val_index]\n",
    "        \n",
    "        train = pd.DataFrame(list(zip(x_tr, y_tr)), \n",
    "                             columns=['statement', 'sentiment'])\n",
    "        val = pd.DataFrame(list(zip(x_va, y_va)), \n",
    "                           columns=['statement', 'sentiment'])\n",
    "\n",
    "        train_ds = create_dataset(train, tokenizer, MAX_LENGTH)\n",
    "        val_ds = create_dataset(val, tokenizer, MAX_LENGTH)\n",
    "        test_ds = create_dataset(test_df, tokenizer, MAX_LENGTH)\n",
    "        \n",
    "\n",
    "        if model_type == 'bert':\n",
    "            model = BERTSentimentClassifier(NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        elif model_type == 'distilbert':\n",
    "            model = DistilBertForSequenceClassification(pretrained_model_name=DISTILBERT_MODEL_NAME, \n",
    "                                                        num_classes=NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        elif model_type == 'roberta':\n",
    "            model = RobertaSentimentClassifier(n_classes=NUM_CLASSES)\n",
    "            model = model.to(device)\n",
    "        \n",
    "        train_loader = create_dataloader(train_ds, BATCH_SIZE)\n",
    "        val_loader = create_dataloader(val_ds, BATCH_SIZE)\n",
    "        test_loader = create_dataloader(test_ds, BATCH_SIZE)\n",
    "        \n",
    "        training_steps = len(train_loader.dataset) * EPOCHS\n",
    "        warmup_steps = int(0.1 * training_steps)\n",
    "        optimizer = AdamW(model.parameters(), \n",
    "                          lr=LEARNING_RATE, \n",
    "                          weight_decay=WEIGHT_DECAY, \n",
    "                          correct_bias=True)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=warmup_steps, \n",
    "                                                    num_training_steps=training_steps)\n",
    "        \n",
    "        model_save_name = '{}_fold_{}.bin'.format(model_type, fold)\n",
    "        \n",
    "        history, preds, outputs = train_fold(epochs=EPOCHS,\n",
    "                                             model=model, \n",
    "                                             device=device, \n",
    "                                             train_dataloader=train_loader, \n",
    "                                             val_dataloader=val_loader,\n",
    "                                             test_dataloader=test_loader,\n",
    "                                             loss_fn=loss_function,\n",
    "                                             optimizer=optimizer,\n",
    "                                             scheduler=scheduler,\n",
    "                                             model_save_name=model_save_name,\n",
    "                                             n_train=len(train),\n",
    "                                             n_val=len(val),\n",
    "                                             single_model=False\n",
    "                                            )\n",
    "        \n",
    "        history_list.append(history)\n",
    "        oof_preds.append(preds)\n",
    "        oof_outputs.append(outputs)\n",
    "        oof_preds_indices.append(val_index)\n",
    "        _, _, test_preds, test_outputs = eval_model(model, \n",
    "                                                    device, \n",
    "                                                    test_loader, \n",
    "                                                    loss_function, \n",
    "                                                    len(test_df))\n",
    "        test_preds_list.append(test_preds)\n",
    "        test_outputs_list.append(test_outputs)\n",
    "        \n",
    "        fold += 1\n",
    "\n",
    "    print(str(NFOLDS), 'Fold CV Train Time: ', format_time(time.time() - start_time))\n",
    "    return history_list, test_outputs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da19c807e4741c8ac897f19f3c97a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=433.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b860ff941ecc4981bef7639ad2ad158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435779157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1 / 5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a99f80245a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                                  num_training_steps=training_steps)\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# train single model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m bert_history, bert_preds, bert_outputs = train_fold(epochs=EPOCHS, \n\u001b[0m\u001b[1;32m     55\u001b[0m                                                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                                     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b6ab3eb3b1b4>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(epochs, model, device, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, scheduler, model_save_name, n_train, n_val, single_model)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         training_output = train_model(model, \n\u001b[0m\u001b[1;32m     16\u001b[0m                                       \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-98b2299bfd97>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcomplete_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define loss function and move to device\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# create folds for k-fold CV (use stratified to preserve class distribution)\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# create tokenizer\n",
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# create datasets and dataloaders\n",
    "bert_train_ds = create_dataset(df_train, bert_tokenizer, MAX_LENGTH)\n",
    "bert_test_ds = create_dataset(df_test, bert_tokenizer, MAX_LENGTH)\n",
    "bert_val_ds = create_dataset(df_val, bert_tokenizer, MAX_LENGTH)\n",
    "\n",
    "bert_train_dataloader = create_dataloader(bert_train_ds, BATCH_SIZE)\n",
    "bert_test_dataloader = create_dataloader(bert_test_ds, BATCH_SIZE)\n",
    "bert_val_dataloader = create_dataloader(bert_val_ds, BATCH_SIZE)\n",
    "\n",
    "# define BERT sentiment classifier model using BertModel as base\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BERTSentimentClassifier, self).__init__()\n",
    "        self.model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(DROPOUT_PROB)\n",
    "        self.output = nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.model(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        return self.output(output)\n",
    "        \n",
    "bert_model = BERTSentimentClassifier(NUM_CLASSES)\n",
    "bert_model = bert_model.to(device)\n",
    "\n",
    "# set up optimizer and scheduler\n",
    "training_steps = len(bert_train_dataloader.dataset) * EPOCHS\n",
    "\n",
    "bert_optimizer = AdamW(bert_model.parameters(), \n",
    "                       lr=LEARNING_RATE, \n",
    "                       weight_decay=WEIGHT_DECAY, \n",
    "                       correct_bias=True)\n",
    "\n",
    "warmup_steps = int(0.1 * training_steps)\n",
    "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
    "                                                 num_warmup_steps=warmup_steps, \n",
    "                                                 num_training_steps=training_steps)\n",
    "# train single model                                                 \n",
    "bert_history, bert_preds, bert_outputs = train_fold(epochs=EPOCHS, \n",
    "                                                    model=bert_model, \n",
    "                                                    device=device, \n",
    "                                                    train_dataloader=bert_train_dataloader, \n",
    "                                                    val_dataloader=bert_val_dataloader,\n",
    "                                                    test_dataloader=bert_test_dataloader,\n",
    "                                                    loss_fn=loss_function,\n",
    "                                                    optimizer=bert_optimizer,\n",
    "                                                    scheduler=bert_scheduler,\n",
    "                                                    model_save_name='bert_best_model.bin',\n",
    "                                                    n_train=len(df_train),\n",
    "                                                    n_val=len(df_val),\n",
    "                                                    single_model=True\n",
    "                                                   )\n",
    "                                                   \n",
    "# train models using 10 fold cross validation\n",
    "bert_history, bert_test_outputs = get_oof_and_test_preds(model_type='bert', \n",
    "                                                         tokenizer=bert_tokenizer, \n",
    "                                                         train_df=df_train_full, \n",
    "                                                         test_df=df_test,\n",
    "                                                         single_model=False)                                                 \n",
    "\n",
    "# evaluate performance of ensemble model created using 10 fold cross validation\n",
    "cv_ensemble_performance(bert_test_outputs, df_test['sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
